{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5.5 Random forest\n",
    "\n",
    "Random forest models are accurate and non-linear models and robust to over-fitting and hence quite popular. They however require hyperparameters to be tuned manually, like the value k in the example above.\n",
    "\n",
    "Building random forest starts by generating a high number of individual decision trees. A single decision tree isn’t very accurate, but many different trees built using different inputs (with bootstrapped inputs, features and observations) enable to explore a broad search space and, once combined, produce accurate models, a technique called bootstrap aggregation or bagging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5.5.1 Decision trees\n",
    "\n",
    "A great advantage of decision trees is that they make a complex decision simpler by breaking it down into smaller, simpler decisions using divide-and-conquer strategy. They basically identify a set of if-else conditions that split data according to the value if the features.\n",
    "\n",
    "library(\"rpart\") ## recursive partitioning\n",
    "m <- rpart(Class ~ ., data = Sonar,\n",
    "           method = \"class\")\n",
    "library(\"rpart.plot\")\n",
    "rpart.plot(m)\n",
    "    \n",
    "    \n",
    "    p <- predict(m, Sonar, type = \"class\")\n",
    "table(p, Sonar$Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Decision trees choose splits based on most homogeneous partitions, and lead to smaller and more homogeneous partitions over their iterations.\n",
    "\n",
    "An issue with single decision trees is that they can grow, and become large and complex with many branches, with corresponds to over-fitting. Over-fitting models noise, rather than general patterns in the data, focusing on subtle patterns (outliers) that won’t generalise.\n",
    "\n",
    "To avoid over-fitting, individual decision trees are pruned. Pruning can happen as a pre-condition when growing the tree, or afterwards, by pruning a large tree.\n",
    "\n",
    "Pre-pruning: stop growing process, i.e stops divide-and-conquer after a certain number of iterations (grows tree at certain predefined level), or requires a minimum number of observations in each mode to allow splitting.\n",
    "\n",
    "Post-pruning: grow a large and complex tree, and reduce its size; nodes and branches that have a negligible effect on the classification accuracy are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:1:4: unexpected numeric constant\n1: 5.5.2\n       ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:1:4: unexpected numeric constant\n1: 5.5.2\n       ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "5.5.2 Training a random forest\n",
    "\n",
    "Let’s return to random forests and train a model using the train infrastructure from caret:\n",
    "\n",
    "set.seed(12)\n",
    "model <- train(Class ~ .,\n",
    "               data = Sonar,\n",
    "               method = \"ranger\") \n",
    "print(model)\n",
    "\n",
    "\n",
    "plot(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The main hyperparameters is mtry, i.e. the number of randomly selected variables used at each split. 2 variables produce random models, while 100s of variables tend to be less random, but risk over-fitting. caret automate the tuning of the hyperparameter using a grid search, which can be parametrised by setting tuneLength (that sets the number of hyperparameter values to test) or directly defining the tuneGrid (the hyperparameter values), which requires knowledge of the model.\n",
    "\n",
    "model <- train(Class ~ .,\n",
    "               data = Sonar,\n",
    "               method = \"ranger\",\n",
    "               tuneLength = 5)\n",
    "set.seed(42)\n",
    "myGrid <- expand.grid(mtry = c(5, 10, 20, 40, 60),\n",
    "                     splitrule = c(\"gini\", \"extratrees\"))\n",
    "model <- train(Class ~ .,\n",
    "               data = Sonar,\n",
    "               method = \"ranger\", \n",
    "               tuneGrid = myGrid,\n",
    "               trControl = trainControl(method = \"cv\",\n",
    "                                       number = 5,\n",
    "                                       verboseIter = FALSE))\n",
    "print(model)\n",
    "    plot(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
