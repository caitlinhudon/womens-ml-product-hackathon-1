{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ROC curve for trees + logistic regression\n",
    "2. Cross validation of linear regression -- see bottom of script\n",
    "3. Better model metrics around linear regression + logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Machine Learning Process\n",
    "\n",
    "* Exploratory data analysis\n",
    "* Build the simplest model\n",
    "* Iterate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Key Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <tr>\n",
    "    <td> <img src=\"confusion_matrix.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    <td> <img src=\"bootstrapping.png\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "    </tr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Packages We'll Use\n",
    "\n",
    "* caret: package for Classification And REgression Training (https://topepo.github.io/caret/index.html)\n",
    "* tidyverse: set of packages for tidy data science (https://www.tidyverse.org)\n",
    "* keras: interface for R into the keras deep learning library (https://keras.rstudio.com)\n",
    "* rpart: package for Recursive Partioning And Regression Trees (https://cran.r-project.org/web/packages/rpart/rpart.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in install.packages(\"tidyverse\", \"skimr\", \"AppliedPredictiveModeling\", :\n",
      "“'lib = \"skimr\"' is not writable”"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in install.packages(\"tidyverse\", \"skimr\", \"AppliedPredictiveModeling\", : unable to install packages\n",
     "output_type": "error",
     "traceback": [
      "Error in install.packages(\"tidyverse\", \"skimr\", \"AppliedPredictiveModeling\", : unable to install packages\nTraceback:\n",
      "1. install.packages(\"tidyverse\", \"skimr\", \"AppliedPredictiveModeling\", \n .     \"keras\", \"modelr\", \"rpart\")",
      "2. stop(\"unable to install packages\")"
     ]
    }
   ],
   "source": [
    "install.packages(\"tidyverse\", \"caret\", \"skimr\", \"AppliedPredictiveModeling\", \"keras\", \"modelr\", \"rpart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alligator Data: https://www.r-bloggers.com/simple-linear-regression-2/\n",
    "* Abalone Data: http://archive.ics.uci.edu/ml/datasets/Abalone\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Resources\n",
    "* DataCamp’s Machine Learning with R skill track (requires paid access).\n",
    "* Useful worked example: https://cfss.uchicago.edu/persp003_linear_regression.html\n",
    "* Modeling in the tidyverse: http://r4ds.had.co.nz/model-basics.html\n",
    "* Exploratory visualizations: https://machinelearningmastery.com/data-visualization-in-r/\n",
    "* https://tutorials.iq.harvard.edu/R/Rstatistics/Rstatistics.html\n",
    "* MLR is working towards a scikit-like implementation https://github.com/mlr-org/mlr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# supervised vs. unsupervised\n",
    "\n",
    "[rephrase]\n",
    "In supervised learning (SML), the learning algorithm is presented with labelled example inputs, where the labels indicate the desired output. SML itself is composed of classification, where the output is categorical, and regression, where the output is numerical.\n",
    "\n",
    "In unsupervised learning (UML), no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5.6 Data pre-processing\n",
    "\n",
    "5.6.1 Missing values\n",
    "\n",
    "Real datasets often come with missing values. In R, these should be encoded using NA. There are basically two approaches to deal with such cases.\n",
    "\n",
    "Drop the observations with missing values, or, if one feature contains a very high proportion of NAs, drop the feature altogether. These approaches are only applicable when the proportion of missing values is relatively small. Otherwise, it could lead to loosing too much data.\n",
    "\n",
    "Impute missing values.\n",
    "\n",
    "Data imputation can however have critical consequences depending on the proportion of missing values and their nature. From a statistical point of view, missing values are classified as missing completely at random (MCAR), missing at random (MAR) or missing not at random (MNAR), and the type of the missing values will influence the efficiency of the imputation method.\n",
    "\n",
    "The figure below shows how different imputation methods perform depending on the proportion and nature of missing values (from Lazar et al., on quantitative proteomics data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We could perform imputation manually, but caret provides a whole range of pre-processing methods, including imputation methods, that can directly be passed when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We have seen in the Unsupervised learning chapter how data at different scales can substantially disrupt a learning algorithm. Scaling (division by the standard deviation) and centring (subtraction of the mean) can also be applied directly during model training by setting. Note that they are set to be applied by default prior to training.\n",
    "\n",
    "train(X, Y, preProcess = \"scale\")\n",
    "train(X, Y, preProcess = \"center\")\n",
    "As we have discussed in the section about Principal component analysis, PCA can be used as pre-processing method, generating a set of high-variance and perpendicular predictors, preventing collinearity.\n",
    "\n",
    "train(X, Y, preProcess = \"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5.7.1 Multiple pre-processing methods\n",
    "\n",
    "It is possible to chain multiple processing methods: imputation, center, scale, pca.\n",
    "\n",
    "train(X, Y, preProcess = c(\"knnImpute\", \"center\", \"scale\", \"pca\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recipes and baking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To deal with the dummy variable issue, we can expand the recipe with more steps:\n",
    "mod_rec <- recipe(Sale_Price ~ Longitude + Latitude + Neighborhood, data = ames_train) %>%\n",
    "step_log(Sale_Price, base = 10) %>%\n",
    "# Lump factor levels that occur in <= 5% of data as \"other\"\n",
    "step_other(Neighborhood, threshold = 0.05) %>%\n",
    "# Create dummy variables for _any_ factor variables\n",
    "step_dummy(all_nominal())\n",
    "Note that we can use standard dplyr selectors as well as some new ones based on the data type\n",
    "( all_nominal() ) or by their role in the analysis ( all_predictors() )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Now that we have a preprocessing speci􀂀cation, let's run it on the training set to prepare the recipe:\n",
    "mod_rec_trained <- prep(mod_rec, training = ames_train, retain = TRUE, verbose = TRUE)\n",
    "## oper 1 step log [training]\n",
    "## oper 2 step other [training]\n",
    "## oper 3 step dummy [training]\n",
    "Here, the \"training\" is to determine which factors to pool and to enumerate the factor levels of the\n",
    "Neighborhood variable,\n",
    "retain keeps the processed version of the training set around so we don't have to recompute it.\n",
    "14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Once the recipe is prepared, it can be applied to any data set using bake :\n",
    "ames_test_dummies <- bake(mod_rec_trained,newdata = ames_test)\n",
    "names(ames_test_dummies)\n",
    "## [1] \"Sale_Price\" \"Longitude\" \"Latitude\"\n",
    "## [4] \"Neighborhood_College_Creek\" \"Neighborhood_Old_Town\" \"Neighborhood_Edwards\"\n",
    "## [7] \"Neighborhood_Somerset\" \"Neighborhood_Northridge_Heights\" \"Neighborhood_Gilbert\"\n",
    "## [10] \"Neighborhood_Sawyer\" \"Neighborhood_other\"\n",
    "If retain = TRUE the training set does not need to be \"rebaked\". The juice function can return the\n",
    "processed version of the training data.\n",
    "Selectors can be used with bake and the default is everything() ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training vs. test set\n",
    "\n",
    "We typically split data into training and test data sets:\n",
    "Training Set: these data are used to estimate model parameters and to pick the values of the\n",
    "complexity parameter(s) for the model.\n",
    "Test Set: these data can be used to get an independent assessment of model ef􀁿cacy. They should\n",
    "not be used during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(rsample)\n",
    "# Make sure that you get the same random numbers\n",
    "set.seed(4595)\n",
    "data_split <- initial_split(ames, strata = \"Sale_Price\")\n",
    "ames_train <- training(data_split)\n",
    "ames_test <- testing(data_split)\n",
    "nrow(ames_train)/nrow(ames)\n",
    "\n",
    "\n",
    "\n",
    "library(ggplot2)\n",
    "## Do the distributions line up?\n",
    "ggplot(ames_train, aes(x = Sale_Price)) +\n",
    "geom_line(stat = \"density\",\n",
    "trim = TRUE) +\n",
    "geom_line(data = ames_test,\n",
    "stat = \"density\",\n",
    "trim = TRUE, col = \"red\")\n",
    "Outcome Distributions\n",
    "11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5.8.6 Comparing models\n",
    "\n",
    "We can now use the caret::resamples function that will compare the models and pick the one with the highest AUC and lowest AUC standard deviation.\n",
    "\n",
    "model_list <- list(glmmet = glm_model,\n",
    "                   rf = rf_model,\n",
    "                   knn = knn_model,\n",
    "                   svm = svm_model,\n",
    "                   nb = nb_model)\n",
    "resamp <- resamples(model_list)\n",
    "resamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary(resamp)\n",
    "\n",
    "lattice::bwplot(resamp, metric = \"ROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We usually don't have two-dimensional data so a quantitative method for under measuring over􀁿tting is\n",
    "needed. Resampling 􀁿ts that description. A simple method for tuning a model is to used grid search:\n",
    "├── Create a set of candidate tuning parameter values\n",
    "└── For each resample\n",
    "│ ├── Split the data into analysis and assessment sets\n",
    "│ ├── [preprocess data]\n",
    "│ ├── For each tuning parameter value\n",
    "│ │ ├── Fit the model using the analysis set\n",
    "│ │ └── Compute the performance on the assessment set and save\n",
    "├── For each tuning parameter value, average the performance over resamples\n",
    "├── Determine the best tuning parameter value\n",
    "└── Create the final model with the optimal parameter(s) on the training set\n",
    "Random search is a similar technique where the candidate set of parameter values are simulated at\n",
    "random across a wide range. Also, an example of nested resampling can be found here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources / Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:1:7: unexpected '/'\n1: https:/\n          ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:1:7: unexpected '/'\n1: https:/\n          ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "https://lgatto.github.io/IntroMachineLearningWithR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "6.3 Credit\n",
    "\n",
    "Many parts of this course have been influenced by the DataCamp’s Machine Learning with R skill track, in particular the Machine Learning Toolbox (supervised learning chapter) and the Unsupervised Learning in R (unsupervised learning chapter) courses.\n",
    "\n",
    "The very hands-on approach has also been influenced by the Software and Data Carpentry lessons and teaching styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://www.tidyverse.org/\n",
    "R for Data Science\n",
    "Jenny's purrr tutorial or Happy R Users Purrr\n",
    "Programming with dplyr vignette\n",
    "Selva Prabhakaran's ggplot2 tutorial\n",
    "caret package documentation\n",
    "CRAN Machine Learning Task View"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
