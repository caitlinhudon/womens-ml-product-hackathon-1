{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* Using the Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unsupervised learning (UML), no labels are provided, and the learning algorithm focuses solely on detecting structure in unlabelled input data. One generally differentiates between\n",
    "\n",
    "Clustering, where the goal is to find homogeneous subgroups within the data; the grouping is based on distance between observations.\n",
    "\n",
    "Dimensionality reduction, where the goal is to identify patterns in the features of the data. Dimensionality reduction is often used to facilitate visualisation of the data, as well as a pre-processing method before supervised learning.\n",
    "\n",
    "UML presents specific challenges and benefits:\n",
    "\n",
    "there is no single goal in UML\n",
    "there is generally much more unlabelled data available than labelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kmeans clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k-means:\n",
    "* n observations\n",
    "* k clusters (we choose k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats::kmeans(x, centers = 3, nstart = 10)\n",
    "\n",
    "x -> numeric data matrix\n",
    "centers is k (# clusters)\n",
    "nstart is number of times we can repeat process (to improve model\n",
    "                                                \n",
    "                                                \n",
    "cl <- kmeans(x, 3, nstart = 10)\n",
    "plot(x, col = cl$cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "how it works:\n",
    "Initialisation: randomly assign class membership\n",
    "\n",
    "Iteration:\n",
    "\n",
    "Calculate the centre of each subgroup as the average position of all observations is that subgroup.\n",
    "Each observation is then assigned to the group of its nearest centre.\n",
    "\n",
    "\n",
    "Get convergence GIF from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "choosing the best model: (elbow method)\n",
    "ks <- 1:5\n",
    "tot_within_ss <- sapply(ks, function(k) {\n",
    "    cl <- kmeans(x, k, nstart = 10)\n",
    "    cl$tot.withinss\n",
    "})\n",
    "plot(ks, tot_within_ss, type = \"b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "4.5 Principal component analysis (PCA)\n",
    "\n",
    "Dimensionality reduction techniques are widely used and versatile techniques that can be used o\n",
    "\n",
    "find structure in features\n",
    "pre-processing for other ML algorithms, and\n",
    "as an aid in visualisation.\n",
    "The basic principle of dimensionality reduction techniques is to transform the data into a new space that summarise properties of the whole data set along a reduced number of dimensions. These are then ideal candidates used to visualise the data along these reduced number of informative dimensions.\n",
    "\n",
    "4.5.1 How does it work\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique that transforms the original n-dimensional data into a new n-dimensional space.\n",
    "\n",
    "These new dimensions are linear combinations of the original data, i.e. they are composed of proportions of the original variables.\n",
    "Along these new dimensions, called principal components, the data expresses most of its variability along the first PC, then second, …\n",
    "Principal components are orthogonal to each other, i.e. non-correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In R, we can use the prcomp function.\n",
    "\n",
    "Let’s explore PCA on the iris data. While it contains only 4 variables, is already becomes difficult to visualise the 3 groups along all these dimensions.\n",
    "\n",
    "pairs(iris[, -5], col = iris[, 5], pch = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Let’s use PCA to reduce the dimension.\n",
    "\n",
    "irispca <- prcomp(iris[, -5])\n",
    "summary(irispca)\n",
    "## Importance of components:\n",
    "##                           PC1     PC2    PC3     PC4\n",
    "## Standard deviation     2.0563 0.49262 0.2797 0.15439\n",
    "## Proportion of Variance 0.9246 0.05307 0.0171 0.00521\n",
    "## Cumulative Proportion  0.9246 0.97769 0.9948 1.00000\n",
    "A summary of the prcomp output shows that along PC1 along, we are able to retain over 92% of the total variability in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "Density-Based Spatial Clustering of Applications with Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
